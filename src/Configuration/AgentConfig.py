"""
Agent constants used in LLM analysis of resource names.

This module contains all constants used in the agent workflow.
"""
from enum import Enum
from pydantic import BaseModel
from typing import Dict

class ModelProvider(str, Enum):
    """Supported model providers."""
    GEMINI_FLASH = "gemini-2.5-flash"
    GEMINI_PRO = "gemini-2.5-pro"
    GPT4 = "gpt-4"

class ModelConfig(BaseModel):
    """Model-specific configuration."""
    name: ModelProvider
    max_safe_tokens: int
    tokens_per_item: int
    overhead_tokens: int

    @property
    def recommended_batch_size(self) -> int:
        """Calculate recommended batch size for this model."""
        available_tokens = self.max_safe_tokens - self.overhead_tokens
        return available_tokens // self.tokens_per_item if self.tokens_per_item > 0 else 0



class AgentConfig:
    """Constants used in the agent workflow."""

    # --- Agent Settings ---
    AGENT_UNIQUE_NAMES_PATH: str = 'unique_resources_names-2024-06.csv'
    """Path to the CSV file containing unique resource names for the agent to process."""
    AGENT_PROTECT_SET_PATH: str = 'protect_set_combined.2024_06.p2.csv'
    """Path to the protect set CSV file used by the agent's knowledge base."""
    AGENT_OUTPUT_DIR: str = 'agent'
    """Directory to store the agent's output files."""
    AGENT_SUGGESTED_ENTITIES_PATH: str = 'suggested_entities.yml'
    """Path to the YAML file where the agent stores suggested entities."""
    AGENT_MASKED_NAMES_INPUT_DIR: str = 'agent_inputs'
    """Directory for the CSV file containing original and masked resource names, generated by the audit workflow."""
    AGENT_RESIDUE_CLEANING_REGEX: str = r'[-_\/]'
    """Regex pattern to remove delimiters and noise from residue strings."""
    AGENT_MIN_MEANINGFUL_RESIDUE_LENGTH: int = 2
    """The minimum character length for a residue to be considered meaningful for LLM analysis."""
    AGENT_DEFAULT_MODEL: ModelProvider = ModelProvider.GEMINI_FLASH
    """The default LLM model to use for entity extraction."""
    AGENT_EXTRACTION_DESCRIPTION: str ="""As an expert in cloud architecture, planning and operations, and a broad understanding of business operations,
    you can help accurately identify business entities from resource names that have already had technical noise removed."""
    AGENT_EXTRACTION_INSTRUCTIONS: str = """
    Using your knowledge of business entities and cloud resource naming conventions:

    1. The input strings are resource names chunks that remain after heuristic analysis for business entity discovery.
    2. ANALYZE the input strings for business entities.
    3. IGNORE any remaining technical terms (api, vnet, vpn, db, prod, dev, sql, etc.), regions names or codes, and environments.
    4. EXTRACT meaningful business entities.
    5. GENERATE 3-5 letter abbreviations for each entity (assuming 5 is practical).
    6. Structure your response as a collection where each item contains: the original input string exactly as provided, followed by the business entities extracted from that string, and for each entity include its name paired with a list of the generated abbreviations.

    Rules:
    - Always provide at least one 3-letter abbreviation per entity
    - Treat numbers as part of entity names when relevant (e.g., "365" in "office365")
    - For compound concepts that represent a single business unit, keep together
    - For separate business functions, split into individual entities
    - Always respond only with the output data structure.
   """

    MODEL_CONFIGS: Dict[ModelProvider, ModelConfig] = {
        ModelProvider.GEMINI_FLASH: ModelConfig(
            name=ModelProvider.GEMINI_FLASH,
            max_safe_tokens=50_000, # Well below your 200k observation
            tokens_per_item=20,      # ~5 input + ~15 output per item
            overhead_tokens=500
        ),
        ModelProvider.GEMINI_PRO: ModelConfig(
            name=ModelProvider.GEMINI_PRO,
            max_safe_tokens=30_000,
            tokens_per_item=20,
            overhead_tokens=500
        ),
        ModelProvider.GPT4: ModelConfig(
            name=ModelProvider.GPT4,
            max_safe_tokens=6_000,
            tokens_per_item=20,
            overhead_tokens=500
        ),
    }

    @classmethod
    def get_model_config(cls, provider: ModelProvider) -> ModelConfig:
        """Return the model configuration registered for the given provider."""
        return cls.MODEL_CONFIGS[provider]


class ResponseSchema:
    """Defines the JSON schema for validating the LLM's extraction response."""
    SCHEMA = {
        "type": "array",
        "items": {
            "type": "object",
            "properties": {
                "chunk": {"type": "string"},
                "entities": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "entity_name": {"type": "string"},
                            "abbreviations": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["entity_name", "abbreviations"]
                    }
                }
            },
            "required": ["chunk", "entities"]
        }
    }